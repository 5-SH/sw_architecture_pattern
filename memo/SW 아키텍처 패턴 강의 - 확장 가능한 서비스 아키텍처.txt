[1부 서비스, 이중화, 분할]
	핵심 원칙들
		가용성
		성능 : 빠른 응답 시간과 낮은 레이턴시
		신뢰성
	
	wirte read 분리
		APi 서버 만들때 Read Write 도메인을 분리해야함 -> CQRS
		Read 가 많은 서비스 인지 Write 가 많은 서비스 인지에 따라
		서비스 구조가 많이 달라짐
	
		백엔드 전통적인 구조
			OLAP : wirte 많음
			OLTP : read 많음
	
	백엔드 정책
		고 가용성
		서비스 서버 -> 이중화
		디바이스 서버 -> 빠르게 복구
		이중화 했다는 가정하에 API 서버의 성능은 50% 만 써야함
			다른 서버가 죽으면 한 대가 모두 처리 해야하기 때문
		Read 작업은 Slave 
		Write 작업은 Master 에서 해야 Lock 이 걸리지 않음
		데이터 이중화
		데이터 동기화는?
			Shared nothing
			Shared disk -> 장애의 원인은 디스크 이므로 좋은 방법
				디스크 IO 가 많은 서비스는 클라우드 보다 베어메탈 서버를 쓰는것이 
				비용적으로 훨씬 낫다
			Shared memory
				Line 에서 이 구조를 사용
				메모리에서 모든 것을 처리
				거대한 공유메모리로 사용
	
	Shared nothing 
		클라우드 환경에서 주로 사용 -> 요즘 트렌드
		작은 인스턴스를 쪼개서 나눠씀
		40G 1대 대신 4G 10대로 구성
		쪼개서 장애없이 어떻게 만들 것이냐가 관점
		
	Redundancy 
		
	Partitioning
		디스크가 견딜 수 있는 I/O 양 때문에 나눔 
		Vertical
			세로로 자르는 이유는 read 랑 write 가 한 테이블에 발생해서
			테이블에 lock 이 발생할까봐 락을 피하기 위해 vertical 로 자름
			테이블에 락이 잘 안걸리던데요?
				mysql oracle 은 테이블 앞에 버퍼구간이 있어서 알아서 캐시를 제공함 그래서 캐시를 조회함 
				그러나 postgres 나 mssql 는 락에 매우 민감함
		Horizontal = sharding
			테이블에서 row 를 range 에 따라서 나눔
			DBA 역할 : 하나의 테이블이 너무 커서 IO 를 못견디면 테이블을 쪼개거나 뷰로 묶음
		쪼개는 방법
			ID 값으로 나누는 방법(id % 4) 
				4개로 쪼개다가 6개로 쪼개져야 한다면 관리가 안 됨
			=> range based partitioning
				범위에 대한 키 정보는 redis 같은 캐쉬 서버에 넣고
				범위 값에 따라 쪼갬
				로컬리티가 있음 => 어떤 값 인근에 어떤 값과 가까운 값이 있다는 직관이 가능
				주로 사용함
			키 또는 해쉬 값으로 쪼갬
				=> 몽고디비 : 확장성을 고려한 디비
					row, column 을 저장하는 것이 아니고 json 오브젝트를 저장함
					몽고디비가 알아서 해쉬 값 기준으로 쪼개준다
				잘 안쓰는 이유 : 직접 range based 로 쓰는것 보다 느리다
				write 속도가 빠르고 read 속도가 느리다
					mysql 이 몽고보다 read 2배 빠르다
					몽고는 mysql 보다 write 가 3배 빠르다
			Directory Based Partitioning
			Consistent Hashing(Hash Ring)
				동일한 데이터를 여러 군데에 저장
				동일한 데이터를 여러 군데에 씀 -> 하나가 죽으면 다른 곳의 데이터를 쓰면됨
				같은 것에 대한 값이 다르면 많은 값을 따르도록 함
				트랜잭션은 포기해야 함
				페이스북 처럼 꼭 신뢰성있는 데이터를 안보여줘도 될 때(ex. 게시판)
				카산드라, 다이나모 디비

[2부 빠르고 확장 가능 데이터 접근]
	디스크 read write 문제가 대부분임
		어디엔가 로그를 남겨둔다
		디스크 IO 를 죽인다 식으로 해결
			=> 캐시를 이용해 IO 를 낮춤
	캐시를 어디에 둬야 하나?
		DB 앞에
		노드마다 캐시를 두면(분산 캐시) 문제가 생긴다
			캐시 미스가 많이 발생함
			그래서 대부분 라우터는 ip hashing 을 사용함
				=> ip hashing 값은 촘촘하지 않아서 부하가 골고루 나눠지지 않음
				=> 라운드 로빈 방법을 사용함(이러면 캐시가 계속 미스난다)
			=> Round Robin Persistence 정책
				기본적으로 라운드로빈으로 가지만 예정 사용자가 방문하면 예정에 갔던 곳으로 보내줌
	캐시 두는 두가지 방법
		Global Cache
			A 타입
				모든 사용자가 공통적으로 접근하는 자료(처음 뜨는 화면)는 전역 캐시에서 가져옴
			B 타입
				사용자마다 요청이 다른 경우
					캐시에 데이터가 있는지 확인하고 
					없으면 DB 에서 가져오고 있으면 캐쉬에 있는 값을 가져옴
		분산 캐시
			A, B, C, D 지역이 다른 경우 사용
				지역마다 관심사가 다르기 때문
			관심사가 분리될 경우
			CDN 의 분산 캐시
	
	캐시에 데이터가 없을 때
		처음에 트래픽이 많이 들어오면 트래픽이 확 튐
			캐쉬 같은 곳에 자료가 없어서
			=> warm start
	
	분산 캐시 예
		레디스 vs memcached
		레디스가 메우 중요
			페이스북 KPPG 에서 레디스 검색
			자료구조를 가진 캐시
		레디스를 어떻게 쓰냐에 따라 백엔드 성능이 매우 달라짐
			sorted set 자주 씀
			HyperLogLog 
				MAU, DAU 뽑을때 -> DB 풀스캔 -> HyperLogLog 를 사용하면 됨
			세션키 관리에 레디스를 유용하게 쓸 수 있음
				데이터 베이스 샤딩 정보를 세션키 값에 붙여야 함
					=> 레디스에 샤딩 키 값을 또 물어볼 필요 없이 한 번에 해결할 수 있음
			집계 정보 만들때 레디스 자주 사용함
			
	프록시 
		Reverse Proxy(로드 밸런서)
			주로 사용하는 오픈소스
				Haproxy
				Nginx	
	
	인덱스 
		DBMS 상태 대시보드 - exem

[3. 그 외 요소들]
	로드 밸런서
		로드 밸런서 뒤에 로드 밸런서를 더 둘 수 있다
	동기적 요청 처리 상황
		Net Funnel 
			추석 코레일 5분 이후 접속 가능합니다
			사용자 대기 큐
		half-sync /half-async 패턴
			프린터 스풀링(async blocking)
		대용량 데이터 처리(ex.분석 서비스, 호텔스 닷컴) 는 클라이언트 요청을 모두 메시지 큐에 받고
		클라이언트는 메시지 큐에 요청을 추가하면 200 응답을 받고 끝난다.
		그리고 서버는 큐에 있는 데이터를 되는대로 처리함.
		ex) RabbitMQ, kafka, active mq, redis
		엄청나게 많은 데이터가 들어오면 우선 큐에 넣는다 -> 되는대로 큐에서 꺼내서 처리한다
			-> 큐에서 데이터 꺼낼때마다 db write 하면 db 락이 걸릴 수 있다
			-> 실시간으로 데이터가 들어가지 않아도 된다면 레디스에서 데이터를 일정시간 모은 다음
			한 번에 DB 쓰기 처리하도록 한다
		RabbitMQ 는 클러스터 명령으로 여러대 묶어 쓸 수 있다
			3대 까지 가능하고 3대 중 1대가 죽으면 2대가 돌아간다
			
[4. 데이터베이스]
	MySQL, MSSQL
		MySQL 은 read write 를 파일에 바로하는 것이 아니고 
		메모리의 버퍼 풀에 먼저 작업하고 모아서 한 번에 파일에 처리
		MSSQL 은 버퍼 풀을 안 씀 -> MySQL 처럼 써버리면 락이 바로 걸림
			MSSQL 는 partitioned view 라고 해서 샤딩 개념을 쉽게 할 수 있도록 되어 있음
			테이블이 쪼개서 저장되어 있고 view 로 묶어놨기 때문에 MySQL 과 MySQL 과 설계가 매우 다르다
			
	레플리케이션
		write 는 마스터 read 는 slave
		대부분의 요청은 read 
		읽기 요청이 많이 오면 slave 를 늘려서 대응할 수 있다
		ProxySQL : 디비의 로드 밸런서
		
	NoSQL 
		왜 나왔는가?
			Scalability 가 매우 좋다
			다른 디비는 개발자가 직접 다 해줘야 함
			
		CAP 이론 
			C : 여러 서버의 데이터가 항상 동일함을 보장할 것이냐
			A : 여러개 서버 중 하나가 죽어도 동작하게 할거냐
			P: 일시적인 장애에도 정상적으로 동작하게 할것이냐
			=> 디비는 두 가지 밖에 만족 못함
				CA - RDBMS
				CP - MongoDB, Reids
				AP - 카산드라, 다이나모
					write 에 강하고 read 에 약하다
				기본 속성이 이런거지 CA 디비도 설정으로 CP 나 AP 식으로 쓸 수 있다
				
	MSA 패턴
		MSA 를 제대로 하려면 인력이 많아야 하고 갈려나가야됨
		각각의 서비스들이 high ability -> 개발이 어려움
		운영이 어려움
		=> 이전으로 다시 돌아가는 추세
		각 서비스는 비즈니스로직에 특화된 로직을 써라
		Saga
			트랜잭션을 일관성을 유지하도록 잘 써야 한다
		CQRS
			읽기와 쓰기를 분리해라
		EventSourcing
			데이터를 스트림으로 처리하겠다
		BFF
			프론트엔드를 고려한 백엔드를 만들어라
			프론트엔드가 해상도 정보를 주고 서버에 요청을 하면 
			프론트가 보여줄 수 있는 만큼만 응답
		
		
		
	